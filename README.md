# LLM-Inference-Using-Groq
Contains code to use meta llama models using groq. 

- [Groq](https://groq.com/) uses their own chip LPU (Language Processing Unit) to get an LLM response.
- It gave the fastest inference on 70B LLaMA I have ever seen.
- All you have to do is have their free tier api and you are good to go.
