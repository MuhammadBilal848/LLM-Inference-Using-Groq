# LLM-Inference-Using-Groq
Contains code to use open-source models using groq. 

- [Groq](https://groq.com/) uses their own chip LPU (Language Processing Unit) to get an LLM response.
- It gave the fastest inference on 70B LLaMA I have ever seen.
- All you have to do is have their free tier api and you are good to go.

## Google:
  - gemma-7b-it
  - gemma2-9b-it

## Meta:
  - llama3-groq-70b-8192-tool-use-preview
  - llama3-groq-8b-8192-tool-use-preview
  - llama-3.1-70b-versatile
  - llama-3.1-8b-instant
  - llama-3.2-11b-text-preview
  - llama-3.2-11b-vision-preview
  - llama-3.2-1b-preview
  - llama-3.2-3b-preview
  - llama-3.2-90b-text-preview
  - llama-guard-3-8b
  - llama3-70b-8192
  - llama3-8b-8192
